#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import pandas as pd
import numpy as np    
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.feature_selection import RFECV
from sklearn.metrics import precision_recall_fscore_support
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
#param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

def RandomForestModel(X_train, X_test, y_train, y_test):

    clf = RandomForestClassifier(random_state = 42)
    # rfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(5, shuffle=True, random_state=1), scoring='roc_auc')

    # param_grid = { 
    #     'estimator__n_estimators': [10, 20, 30],
    #     # 'estimator__max_features': ['sqrt', 'log2', None],
    #     # 'estimator__class_weight': ['balanced', 'balanced_subsample'],
    #     'estimator__max_depth': [5, 10, 15],
    #     'estimator__criterion': ['gini', 'entropy']
    #     # 'estimator__min_samples_split': [2, 5, 10, 15],
    #     # 'estimator__min_samples_leaf': [1, 2, 5, 10]
    # }

    param_grid = { 
        'n_estimators': [10, 20, 30],
        'max_features': ['sqrt', 'log2', None],
        'class_weight': ['balanced', 'balanced_subsample'],
        'max_depth': [5, 10, 15],
        'criterion': ['gini', 'entropy'],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 5, 10]
    } 

    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

    #------------- Just pass your RFECV object as estimator here directly --------#

    # CV_rfc = GridSearchCV(estimator=rfecv, param_grid=param_grid, cv= k_fold, scoring = 'roc_auc', return_train_score=True)
    CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv=k_fold, scoring = 'roc_auc', return_train_score=True)

    CV_rfc.fit(X_train, y_train)
    predicted_labels = CV_rfc.predict(X_test)

    sk_report = classification_report(
        digits=6,
        y_true=y_test, 
        y_pred=predicted_labels)

    with open(os.path.join(output_path, 'model_statistics.txt'), 'w+') as s:
        s.write("Grid scores on development set: \n")
        means = CV_rfc.cv_results_['mean_test_score']
        stds = CV_rfc.cv_results_['std_test_score']
        for mean, std, params in zip(means, stds, CV_rfc.cv_results_['params']):
            s.write("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
                
        s.write("\n\n")           

        s.write('Classification report: \n{}'.format(sk_report))
          
    
    return CV_rfc    

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        # with open(param_path, 'r') as tc:
        #    trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))

        raw_data = pd.concat([ pd.read_csv(file, header=None) for file in input_files ])  
        
        if 'Unnamed: 0' in raw_data.columns:
            raw_data.drop(columns='Unnamed: 0', inplace=True)
        
        raw_data.dropna(inplace=True)               

        # 'signup' is the target field and is the first column
        X, y = raw_data.iloc[:,1:], raw_data.iloc[:,0]        

        # Splitting data into training and validation set
        train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.2, 
                                                            stratify=y, random_state = 88)

        # Here we only support a single hyperparameter. Note that hyperparameters are always passed in as
        # strings, so we need to do any necessary conversions.
        # max_leaf_nodes = trainingParams.get('max_leaf_nodes', None)
        # if max_leaf_nodes is not None:
        #     max_leaf_nodes = int(max_leaf_nodes)

        # # Now use scikit-learn's decision tree classifier to train the model.
        # clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
        # clf = clf.fit(train_X, train_y)

        clf = RandomForestModel(train_X, test_X, train_Y, test_Y)
        
        # save the model
        with open(os.path.join(model_path, 'decision-tree-model.pkl'), 'w') as out:
            pickle.dump(clf, out)
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
